{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "THRESHOLD = 5\n",
    "SAMPLE_LENGTH = 1000\n",
    "\n",
    "STRIDE_SIZE = 10.\n",
    "FRAME_SIZE = 25.\n",
    "N_MFCC = 16\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "SONG_DIR = home + \"/Downloads/songdata_90/songdata/\"\n",
    "\n",
    "def get_mfcc_features(song_data, sample_rate, stride_size = STRIDE_SIZE, frame_size = FRAME_SIZE):\n",
    "    mfccs = librosa.feature.mfcc(song_data, sample_rate, \n",
    "                                 n_mfcc=N_MFCC,\n",
    "                                 hop_length=int(STRIDE_SIZE / 1000. * sample_rate), \n",
    "                                 n_fft=int(FRAME_SIZE / 1000. * sample_rate))\n",
    "    return mfccs\n",
    "\n",
    "def load_song(song_file):\n",
    "    data, samplerate = sf.read(song_file)\n",
    "    data = data[:, 0]\n",
    "    mfccs = get_mfcc_features(data, samplerate)\n",
    "    mfccs = np.asarray(mfccs).T\n",
    "    return mfccs\n",
    "\n",
    "def peaks_to_windows_flat(song_name, good_peaks, bad_peaks):\n",
    "    mfccs = load_song(song_name)\n",
    "    good_samples = []\n",
    "    bad_samples = []\n",
    "    for p in good_peaks:\n",
    "        n = int((p[0] - (FRAME_SIZE/1000.)) / (STRIDE_SIZE/1000.))\n",
    "        features = mfccs[n-(SAMPLE_LENGTH//2):n+(SAMPLE_LENGTH//2), :]\n",
    "        good_samples.append(features)\n",
    "    for p in bad_peaks:\n",
    "        n = int((p[0] - (FRAME_SIZE/1000.)) / (STRIDE_SIZE/1000.))\n",
    "        features = mfccs[n-(SAMPLE_LENGTH//2):n+(SAMPLE_LENGTH//2), :]\n",
    "        bad_samples.append(features)\n",
    "    good_samples = np.concatenate(good_samples)\n",
    "    bad_samples = np.concatenate(bad_samples)\n",
    "    return good_samples, bad_samples\n",
    "\n",
    "def peaks_to_windows_mat(song_name, good_peaks, bad_peaks):\n",
    "    mfccs = load_song(song_name)\n",
    "    good_samples = []\n",
    "    bad_samples = []\n",
    "    for p in good_peaks:\n",
    "        n = int((p[0] - (FRAME_SIZE/1000.)) / (STRIDE_SIZE/1000.))\n",
    "        features = mfccs[n-(SAMPLE_LENGTH//2):n+(SAMPLE_LENGTH//2), :]\n",
    "        good_samples.append(features)\n",
    "    for p in bad_peaks:\n",
    "        n = int((p[0] - (FRAME_SIZE/1000.)) / (STRIDE_SIZE/1000.))\n",
    "        features = mfccs[n-(SAMPLE_LENGTH//2):n+(SAMPLE_LENGTH//2), :]\n",
    "        bad_samples.append(features)\n",
    "    good_samples = np.asarray(good_samples)\n",
    "    bad_samples = np.asarray(bad_samples)\n",
    "    return good_samples, bad_samples\n",
    "\n",
    "def get_performance_measures(Y_pred, Y_test):\n",
    "    diffs = [x == y for x, y in zip(list(Y_pred), list(Y_test))]\n",
    "    print(\"Accuracy:\", diffs.count(True) / len(diffs))\n",
    "\n",
    "    recall = [x==1 and y==1 for x, y in zip(Y_pred, Y_test)].count(True) / list(Y_test).count(1)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    precision = [x==1 and y==1 for x, y in zip(Y_pred, Y_test)].count(True) / list(Y_pred).count(1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"F1:\", 2 * recall * precision / (recall + precision))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ariana_Grande___Side_To_Side_(TRU_Concept_Remix_ft._Romany)\n",
      "['170.1']\n"
     ]
    }
   ],
   "source": [
    "results_file = open('resultsFile', 'rb')      \n",
    "results = pickle.load(results_file) \n",
    "list.sort(results, key = lambda x : x[0])\n",
    "\n",
    "df = pd.read_csv(\"songs_fixed.csv\")\n",
    "\n",
    "detection_count = 0\n",
    "drop_count = 0\n",
    "bad_samples_overall = []\n",
    "good_samples_overall = []\n",
    "\n",
    "for i in range(len(results)):\n",
    "    drops = str(df.iloc[[i]][\"Drops\"][i]).split(\", \")\n",
    "    peaks = results[i][1]\n",
    "    good_peaks = set()\n",
    "    temp_detection_count = 0\n",
    "    songname = df.iloc[[i]][\"Song Name\"][i]\n",
    "    drops_found = []\n",
    "    for d in drops:\n",
    "        for p in peaks:\n",
    "            dval = float(d)\n",
    "            pval = float(p[0])\n",
    "            if pval >= dval - 7.5 and pval <= dval + 1.5:\n",
    "                temp_detection_count += 1\n",
    "                drops_found.append(d)\n",
    "                good_peaks.add(p)\n",
    "                break\n",
    "    if temp_detection_count < len(drops):\n",
    "        print(songname)\n",
    "        print(drops_found)\n",
    "    bad_peaks = set(peaks) - good_peaks\n",
    "    drop_count += len(drops)\n",
    "    detection_count += temp_detection_count\n",
    "    \n",
    "    good_samples, bad_samples = peaks_to_windows_mat(SONG_DIR + \"/\" + results[i][0], good_peaks, bad_peaks)\n",
    "    for b in bad_samples:\n",
    "        bad_samples_overall.append(b)\n",
    "        \n",
    "    for g in good_samples:\n",
    "        good_samples_overall.append(g)\n",
    "\n",
    "print(detection_count / drop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn import hmm\n",
    "\n",
    "# TEST_SIZE = 40\n",
    "\n",
    "# model = hmm.GaussianHMM(n_components=5)\n",
    "# lengths = [SAMPLE_LENGTH] * (int((np.shape(good_samples_overall)[0]) / SAMPLE_LENGTH) - TEST_SIZE)\n",
    "# train_sample_count = int(np.shape(good_samples_overall)[0] - TEST_SIZE * SAMPLE_LENGTH)\n",
    "# print(np.shape(lengths), train_sample_count)\n",
    "# model = model.fit(np.asarray(good_samples_overall)[: train_sample_count], lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.score(np.asarray(good_samples_overall)[: train_sample_count], lengths)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn\n",
    "\n",
    "TEST_SIZE = 250\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "good_flat = []\n",
    "for g in good_samples_overall:\n",
    "    good_flat.append(g.flatten())\n",
    "bad_flat = []\n",
    "for b in bad_samples_overall:\n",
    "    bad_flat.append(b.flatten())\n",
    "\n",
    "X = np.concatenate([good_flat, bad_flat])\n",
    "Y = np.concatenate([[1] * np.shape(good_samples_overall)[0], [0] * np.shape(bad_samples_overall)[0]])\n",
    "\n",
    "X, Y = unison_shuffled_copies(X, Y)\n",
    "print(np.shape(X), np.shape(Y))\n",
    "\n",
    "X_test = X[:TEST_SIZE]\n",
    "Y_test = Y[:TEST_SIZE]\n",
    "\n",
    "X_train = X[TEST_SIZE:]\n",
    "Y_train = Y[TEST_SIZE:]\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-2, hidden_layer_sizes=(100, 10), random_state=30, max_iter = 200)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_NN = clf.predict(X_test)\n",
    "Y_probs_NN = clf.predict_proba(X_test)\n",
    "get_performance_measures(Y_pred_NN, Y_test)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=25)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_RFC = clf.predict(X_test)\n",
    "Y_probs_RFC = clf.predict_proba(X_test)\n",
    "get_performance_measures(Y_pred_RFC, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=20, solver='lbfgs',multi_class='multinomial', max_iter = 1000)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_LR = clf.predict(X_test)\n",
    "Y_probs_LR = clf.predict_proba(X_test)\n",
    "get_performance_measures(Y_pred_LR, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_votes = Y_pred_NN + Y_pred_RFC + Y_pred_LR\n",
    "Y_pred_votes = np.asarray([1 if x > 1 else 0 for x in Y_pred_votes])\n",
    "get_performance_measures(Y_pred_votes, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probs = Y_probs_NN * 1.2 + Y_probs_RFC + Y_probs_LR\n",
    "Y_probs_pred = [1 if x[0] < x[1] else 0 for x in Y_probs]\n",
    "get_performance_measures(Y_probs_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise BaseException\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "Y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "get_performance_measures(Y_pred_NN, Y_test)\n",
    "\n",
    "correct_probs = []\n",
    "incorrect_probsFP = []\n",
    "incorrect_probsFN = []\n",
    "for i in range(len(Y_pred)):\n",
    "    if Y_pred[i] != Y_test[i] and Y_pred[i] == 1:\n",
    "        incorrect_probsFP.append(abs(Y_prob[i][0] - Y_prob[i][1]))\n",
    "    elif Y_pred[i] != Y_test[i] and Y_pred[i] == 0:\n",
    "        incorrect_probsFN.append(abs(Y_prob[i][0] - Y_prob[i][1]))\n",
    "    else:\n",
    "        correct_probs.append(abs(Y_prob[i][0] - Y_prob[i][1]))\n",
    "\n",
    "# print(incorrect_probsFP)\n",
    "# print(incorrect_probsFN)\n",
    "# print(correct_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(degree=5)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "X = np.concatenate([good_samples_overall, bad_samples_overall])\n",
    "Y = np.concatenate([[1] * np.shape(good_samples_overall)[0], [0] * np.shape(bad_samples_overall)[0]])\n",
    "\n",
    "X, Y = unison_shuffled_copies(X, Y)\n",
    "\n",
    "X_test = X[:TEST_SIZE]\n",
    "Y_test = Y[:TEST_SIZE]\n",
    "\n",
    "X_train = X[TEST_SIZE:]\n",
    "Y_train = Y[TEST_SIZE:]\n",
    "input_shape = np.shape(X_train)\n",
    "input_shape = (input_shape[1], input_shape[2], 1)\n",
    "num_classes = 2\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=num_classes, dtype='float32')\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), strides=(1, 1),\n",
    "                 input_shape=input_shape))\n",
    "model.add(BatchNormalization(trainable=True))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5, name='dropout2', trainable=False))\n",
    "model.add(Conv2D(64, (2, 2)))\n",
    "model.add(BatchNormalization(trainable=True))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5, name='dropout3', trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_shape = np.shape(X_train)\n",
    "X_train = X_train.reshape([train_shape[0], train_shape[1], train_shape[2], 1])\n",
    "test_shape = np.shape(X_test)\n",
    "X_test = X_test.reshape([test_shape[0], test_shape[1], test_shape[2], 1])\n",
    "print(train_shape, np.shape(X_train))\n",
    "model.fit(X_train, Y_train,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
